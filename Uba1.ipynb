{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8gJDJizp4et"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lgUUfQfqCfU"
   },
   "outputs": [],
   "source": [
    "function_df = pd.read_csv('ubiquitin/ubiquitin.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1562879479190,
     "user": {
      "displayName": "Lam Vo",
      "photoUrl": "",
      "userId": "02914051558378699969"
     },
     "user_tz": 300
    },
    "id": "BgLI-3SnqnaG",
    "outputId": "decbd517-175a-427e-e3bb-2c30ba494baa"
   },
   "outputs": [],
   "source": [
    "function_df['Assay/Protocol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are actually 4 assays in this dataframe. I will start with 'Relative E1(Uba1) Enzyme Reactivity: with Limiting E1 (200 nM)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_df = function_df[function_df['Assay/Protocol'] == function_df['Assay/Protocol'].unique()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_df['Data'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0I-YNvSt2sW"
   },
   "outputs": [],
   "source": [
    "function_df['sequence_len'] = function_df['Sequence'].apply(lambda seq: len(seq)) \n",
    "max_len = function_df['sequence_len'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06tBDnNKq0KT"
   },
   "outputs": [],
   "source": [
    "def sequence_to_aa(seq):\n",
    "  return list(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BChHPyycri7T"
   },
   "outputs": [],
   "source": [
    "def get_sequence_aa(df):\n",
    "  df['sequence_aa'] = df['Sequence'].apply(lambda seq:sequence_to_aa(seq))\n",
    "  df['length'] = df['Sequence'].apply(lambda seq:len(seq))\n",
    "  sequence_aa = np.array(df['sequence_aa'])\n",
    "  return sequence_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8nxTIFWro57"
   },
   "outputs": [],
   "source": [
    "sequence_aa = get_sequence_aa(function_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2046,
     "status": "ok",
     "timestamp": 1562879485473,
     "user": {
      "displayName": "Lam Vo",
      "photoUrl": "",
      "userId": "02914051558378699969"
     },
     "user_tz": 300
    },
    "id": "9PT5O2jBr0BJ",
    "outputId": "9f121849-6e8d-4cbd-fd98-106ab33763db"
   },
   "outputs": [],
   "source": [
    "# import neccesary tools from Keras\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iaBEPPhr5q-"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequence_aa)\n",
    "encoded = tokenizer.texts_to_sequences(sequence_aa)\n",
    "vocab_len = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcjcQR75tP17"
   },
   "outputs": [],
   "source": [
    "X = np.array(encoded)\n",
    "y = function_df['Data']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eYw1KV10snV"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, Softmax, multiply, Lambda, Flatten, Activation, RepeatVector, Permute, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# from fit_one_cycle.clr import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_size = 10\n",
    "dense_sizes = [16]\n",
    "LSTM_dropout = 0.0\n",
    "n_epochs = 200\n",
    "adam_lr = [0.001]\n",
    "initializers = ['he_uniform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87902,
     "status": "ok",
     "timestamp": 1562879747208,
     "user": {
      "displayName": "Lam Vo",
      "photoUrl": "",
      "userId": "02914051558378699969"
     },
     "user_tz": 300
    },
    "id": "WQ6PkDDT0-Ny",
    "outputId": "1e8ebf38-10de-4921-fd93-26c93098e73e"
   },
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/4962\n",
    "def get_model(initializer, LSTM_size, dense_size):\n",
    "    input = Input(shape = [max_len])\n",
    "    embedding = Embedding(input_dim = vocab_len, \n",
    "                          output_dim = 20, \n",
    "                          input_length = max_len, \n",
    "                          trainable = True)(input)\n",
    "\n",
    "    activations = Bidirectional(LSTM(LSTM_size, return_sequences = True,\n",
    "                       dropout = LSTM_dropout,\n",
    "                       kernel_initializer = initializer))(embedding)\n",
    "\n",
    "    attention = Dense(1, activation = 'tanh', kernel_initializer = initializer)(activations)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax', name = 'attention_activations')(attention)\n",
    "    attention = RepeatVector(LSTM_size * 2)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    sequence_representation = multiply([activations, attention])\n",
    "    sequence_representation = Lambda(lambda xin: K.sum(xin, axis = -2), output_shape = (LSTM_size * 2,))(sequence_representation)\n",
    "    \n",
    "    sequence_representation = Dense(dense_size, kernel_initializer = initializer)(sequence_representation)\n",
    "    sequence_representation = Dropout(0.2)(sequence_representation)\n",
    "    sequence_representation = LeakyReLU()(sequence_representation)\n",
    "    \n",
    "    # sequence_representation = Dense(8, kernel_initializer = initializer)(sequence_representation)\n",
    "    # sequence_representation = LeakyReLU()(sequence_representation)\n",
    "    # sequence_representation = Dropout(0.2)(sequence_representation)\n",
    "\n",
    "    output = Dense(1, bias_initializer = keras.initializers.Constant(0.7341))(sequence_representation)\n",
    "\n",
    "    model = Model(inputs = [input], outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(adam_lr)):\n",
    "    lr = adam_lr[i]\n",
    "    for j in range(len(initializers)):\n",
    "        initializer = initializers[j]\n",
    "        for k in range(len(dense_sizes)):\n",
    "            dense_size = dense_sizes[k]\n",
    "            print(f'adam_lr - {lr} initializer - {initializer}')\n",
    "\n",
    "            optimizer = Adam(lr = lr)\n",
    "            model = get_model(initializer, LSTM_size, dense_size)\n",
    "            model.compile(optimizer = optimizer,\n",
    "                         loss = 'mean_squared_error')\n",
    "            # early_stopping = EarlyStopping(patience = 300)\n",
    "\n",
    "            folder_path = 'ubiquitin/model_and_checkpoints/' + f'LSTM size - {LSTM_size} epochs - {n_epochs} adam_lr {lr} initializer - {initializer}'\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "            checkpointing = ModelCheckpoint(folder_path + 'weights.{epoch:02d}-{val_loss:.5f}.hdf5',\n",
    "                                   save_best_only = True)\n",
    "            history = model.fit(X_train, y_train, validation_data = (X_val, y_val), \n",
    "                                batch_size = 128, epochs = n_epochs, verbose = 1,\n",
    "                                callbacks = [checkpointing])\n",
    "\n",
    "# save the model summary\n",
    "#         model_json = model.to_json()\n",
    "#         with open(folder_path + '.json', 'w') as file:\n",
    "#             file.write(model_json)\n",
    "# get the Pearson score\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred = np.reshape(y_pred, (1094,))\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "# pearson_score = pearsonr(y_test, y_pred)\n",
    "# print(f'Pearson correlation - {pearson_score}')\n",
    "# spearman_score = spearmanr(y_test, y_pred)\n",
    "# print(f'Spearman correlation - {spearman_score}')\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(pearson_score, open(f'LSTM units - {LSTM_units} epochs - {n_epochs} adam_lr {adam_lr} v3.p', 'wb'))\n",
    "\n",
    "# visualize the training and validation loss curves\n",
    "            plt.figure()\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "            epoch = np.arange(n_epochs)\n",
    "            plt.plot(epoch, loss, label = 'loss')\n",
    "            plt.plot(epoch, val_loss, label = 'val_loss')\n",
    "            plt.title(f'BiLSTM size - {LSTM_size} dense size - {dense_size} LSTM dropout - {LSTM_dropout} epochs - {n_epochs} adam_lr {lr} initializer - {initializer}')\n",
    "            plt.legend()\n",
    "            plt.savefig(folder_path + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Pearson score\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.reshape(y_pred, (-1,))\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "pearson_score = pearsonr(y_val, y_pred)\n",
    "print(f'Pearson correlation - {pearson_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "beta_lactamase_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
